# Neuron Swift ML Framework Rules

## Layer Subclass Creation

When creating a new Layer subclass in the Neuron framework:

### Structure Requirements:
- **Inherit from BaseLayer**: All custom layers should inherit from `BaseLayer`
- **Set encodingType**: Each layer must have a unique `EncodingType` enum case
- **Implement required methods**: Override `forward(tensor:context:)` and encoding/decoding methods
- **Follow naming conventions**: Use descriptive names ending with the operation type (e.g., `Dense`, `Conv2d`, `BatchNormalize`)

### Template Pattern:
```swift
import Foundation
import NumSwift

/// [Brief description of what this layer does]
public final class [LayerName]: BaseLayer {
    // Private properties for layer-specific parameters
    private var [layerSpecificProperty]: [Type]
    
    /// Initializer with layer-specific parameters
    /// - Parameters:
    ///   - [param1]: Description of parameter
    ///   - inputs: Optional input count (required for first layer)
    ///   - initializer: Weight initializer function. Default: `.heNormal`
    ///   - biasEnabled: Boolean for bias application. Default: `false`
    public init([parameters],
                inputs: Int? = nil,
                initializer: InitializerType = .heNormal,
                biasEnabled: Bool = false) {
        
        // Set layer-specific properties
        self.[layerSpecificProperty] = [value]
        
        // Call super with appropriate encoding type
        super.init(inputSize: nil,
                   initializer: initializer,
                   biasEnabled: biasEnabled,
                   encodingType: .[layerTypeCase])
        
        // Set output size based on layer logic
        self.outputSize = TensorSize(array: [outputDimensions])
        
        // Initialize weights if inputs provided
        if let inputs = inputs {
            inputSize = TensorSize(array: [inputs, 1, 1])
            initializeWeights(inputs: inputs)
        }
    }
    
    // Codable implementation
    enum CodingKeys: String, CodingKey {
        case biasEnabled, inputSize, outputSize, weights, biases, [customProperties]
    }
    
    convenience public required init(from decoder: Decoder) throws {
        // Decode layer-specific properties
        let container = try decoder.container(keyedBy: CodingKeys.self)
        // Implementation details...
        self.init([decodedParameters])
        // Set decoded properties...
    }
    
    public override func encode(to encoder: Encoder) throws {
        // Encode layer-specific properties
        var container = encoder.container(keyedBy: CodingKeys.self)
        // Implementation details...
    }
    
    // Core forward pass implementation
    public override func forward(tensor: Tensor, context: NetworkContext) -> Tensor {
        // Implement layer-specific forward pass logic
        // Return transformed tensor
    }
    
    // Weight initialization (if needed)
    private func initializeWeights(inputs: Int) {
        // Initialize weights and biases based on input size
    }
    
    // Input size change handler (if needed)
    public override func onInputSizeSet() {
        // Handle input size changes
    }
}
```

### Key Implementation Points:
1. **Forward Pass**: Must implement the core computation logic
2. **Weight Management**: Handle weight initialization and updates appropriately
3. **Batch Processing**: The base class handles batch processing automatically
4. **Device Support**: Inherit device management from BaseLayer
5. **Codable**: Implement proper encoding/decoding for model persistence

## Sequential Subclass Creation

When creating a Sequential-like trainable model:

### Structure Requirements:
- **Implement Trainable Protocol**: Must conform to `Trainable` protocol
- **Implement Logger Protocol**: Should conform to `Logger` for debugging
- **Layer Management**: Handle layer compilation and forward passes
- **Device Propagation**: Ensure device settings propagate to all layers

### Template Pattern:
```swift
import Foundation
import Logger

/// [Description of the sequential model variant]
public final class [SequentialVariantName]: Trainable, Logger {
    public var logLevel: LogLevel = .low
    public var name: String = "[ModelName]"
    
    public var device: Device = CPU() {
        didSet {
            layers.forEach { layer in
                switch device.type {
                case .cpu:
                    layer.device = CPU()
                case .gpu:
                    layer.device = GPU()
                }
            }
        }
    }
    
    public var isTraining: Bool = true {
        didSet {
            layers.forEach { $0.isTraining = isTraining }
        }
    }
    
    public var batchSize: Int = 1 {
        didSet {
            layers.forEach { $0.batchSize = batchSize }
        }
    }
    
    public private(set) var layers: [Layer] = []
    public var isCompiled: Bool = false
    
    // Model-specific properties
    private var [customProperty]: [Type]
    
    public init([parameters]) {
        // Initialize custom properties
        self.[customProperty] = [value]
    }
    
    // Layer management methods
    public func add(_ layer: Layer) {
        layers.append(layer)
        layer.device = device
        layer.isTraining = isTraining
        layer.batchSize = batchSize
    }
    
    public func compile() {
        // Implement compilation logic
        // Set up layer connections and validate architecture
        isCompiled = true
    }
    
    // Prediction methods
    public func predict(_ data: Tensor, context: NetworkContext) -> Tensor {
        // Implement forward pass through all layers
    }
    
    public func predict(batch: TensorBatch, context: NetworkContext) -> TensorBatch {
        // Implement batch prediction
    }
    
    // Codable implementation
    // ... (similar to Layer pattern)
}
```

## Optimizer Subclass Creation

When creating a new Optimizer:

### Structure Requirements:
- **Inherit from BaseOptimizer**: Use `BaseOptimizer` as the base class
- **Implement optimization algorithm**: Override key optimization methods
- **Handle gradient accumulation**: Work with the gradient accumulator system
- **Support momentum/adaptive learning**: Implement algorithm-specific features

### Template Pattern:
```swift
import Foundation
import NumSwift

/// [Description of the optimization algorithm]
public class [OptimizerName]: BaseOptimizer {
    
    // Algorithm-specific parameters
    private var [param1]: Tensor.Scalar
    private var [param2]: Tensor.Scalar
    
    // State variables for optimization
    private var [stateVar1]: [Tensor.Data] = []
    private var [stateVar2]: [Tensor.Data] = []
    
    public override var trainable: Trainable {
        didSet {
            build() // Rebuild state when trainable changes
        }
    }
    
    public init(_ trainable: Trainable,
                device: Device = CPU(),
                learningRate: Tensor.Scalar,
                batchSize: Int,
                [algorithmParams]) {
        
        // Set algorithm parameters
        self.[param1] = [param1]
        self.[param2] = [param2]
        
        super.init(trainable,
                   device: device,
                   learningRate: learningRate,
                   batchSize: batchSize)
        
        build()
    }
    
    // Build/rebuild optimizer state
    private func build() {
        // Initialize state variables based on trainable layers
        // Reset state arrays
        [stateVar1] = []
        [stateVar2] = []
        
        // Initialize for each layer
        for layer in trainable.layers where layer.trainable {
            // Initialize state for this layer
        }
    }
    
    // Core optimization step
    public override func apply(_ gradients: Tensor.Gradient) {
        // Implement algorithm-specific gradient application
        // Update state variables
        // Apply computed updates to layer weights
        
        super.apply(gradients) // Call parent for common functionality
    }
    
    // Reset optimizer state
    public override func reset() {
        super.reset()
        build() // Rebuild state
    }
}
```

### Key Implementation Points:
1. **State Management**: Properly initialize and maintain algorithm state
2. **Gradient Processing**: Handle gradient clipping and accumulation
3. **Learning Rate**: Support learning rate decay functions
4. **Memory Efficiency**: Manage state variable memory appropriately
5. **Batch Handling**: Work correctly with different batch sizes

## General Guidelines

### Code Style:
- Use descriptive variable and function names
- Add comprehensive documentation comments
- Follow Swift naming conventions
- Use `public` visibility for API classes
- Use `private` for implementation details

### Error Handling:
- Throw appropriate `LayerErrors` for invalid configurations
- Validate input dimensions and parameters
- Provide meaningful error messages

### Testing:
- Create unit tests for new components
- Test with different input sizes and batch sizes
- Verify gradient computations are correct
- Test serialization/deserialization

### Performance:
- Use NumSwift operations for mathematical computations
- Minimize memory allocations in forward passes
- Consider GPU compatibility when implementing operations